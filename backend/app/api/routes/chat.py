"""
Shield AI - Chat Router

Endpoint de chat que integra:
1. Pipeline de anonimizaci√≥n del equipo (sin tocar)
2. LLMClient existente para Grok
3. Sistema de desanonimizaci√≥n propio
"""

from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any
import logging
import time
import json

router = APIRouter(prefix="/chat", tags=["Chat"])
logger = logging.getLogger(__name__)


# === HELPER FUNCTIONS ===

def anonymize_with_existing_map(text: str, existing_mapping: Dict[str, str]) -> str:
    """
    Anonimiza texto usando un mapa existente de anonimizaci√≥n.
    
    IMPORTANTE: El mapa que viene del pipeline tiene formato fake_name -> real_name
    Pero nosotros necesitamos real_name -> fake_name para anonimizar
    
    Args:
        text (str): Texto original a anonimizar
        existing_mapping (Dict[str, str]): Mapa del pipeline (fake -> real)
    
    Returns:
        str: Texto anonimizado usando el mapa existente
    """
    result = text
    
    # INVERTIR EL MAPA: fake -> real a real -> fake
    inverted_map = {real_name: fake_name for fake_name, real_name in existing_mapping.items()}
    
    logger.debug(f"üîÑ Mapa original (fake->real): {existing_mapping}")
    logger.debug(f"üîÑ Mapa invertido (real->fake): {inverted_map}")
    
    # Ordenar por longitud descendente para evitar reemplazos parciales
    # Ejemplo: "Juan Garc√≠a" antes que "Juan" para evitar "Mariela Garc√≠a"
    sorted_items = sorted(inverted_map.items(), key=lambda x: len(x[0]), reverse=True)
    
    for real_data, fake_data in sorted_items:
        if real_data in result:
            result = result.replace(real_data, fake_data)
            logger.debug(f"‚úÖ Reemplazo: '{real_data}' -> '{fake_data}'")
    
    logger.debug(f"Anonimizado con mapa existente: '{text}' -> '{result}'")
    return result

# === REQUEST/RESPONSE MODELS ===

class ChatRequest(BaseModel):
    message: str                                    # Prompt del usuario
    session_id: Optional[str] = None               # Para guardar mapas en Redis
    llm_prompt_template: Optional[str] = None      # Template personalizado para LLM
    model: Optional[str] = "es"                    # Idioma para PII detection
    use_regex: Optional[bool] = True               # Detectar PII con regex
    pseudonymize: Optional[bool] = True            # Usar pseud√≥nimos realistas
    save_mapping: Optional[bool] = True            # Guardar mapa en Redis
    use_realistic_fake: Optional[bool] = True      # Usar datos falsos realistas con Faker (por defecto)

class ChatResponse(BaseModel):
    response: str                    # Respuesta final al usuario
    session_id: Optional[str]        # ID de sesi√≥n usado
    pii_detected: bool              # Si se detect√≥ informaci√≥n sensible
    processing_time: float          # Tiempo total de procesamiento
    anonymized_used: bool           # Si se us√≥ anonimizaci√≥n
    llm_model: str                  # Modelo LLM utilizado

class StreamingChatRequest(BaseModel):
    session_id: str                 # ID de sesi√≥n para streaming
    llm_prompt_template: Optional[str] = None  # Template para LLM

# === MAIN CHAT ENDPOINT ===

# @router.post("/", response_model=ChatResponse)
# async def chat_with_llm(request: ChatRequest):
#     """
#     Endpoint principal de chat que:
#     1. Usa pipeline de anonimizaci√≥n del equipo (sin modificar)
#     2. Env√≠a prompt anonimizado al LLMClient existente
#     3. Usa sistema de desanonimizaci√≥n propio para respuesta final
#     """
#     try:
#         start_time = time.time()
        
#         # ===== PASO 1: ANONIMIZACI√ìN USANDO PIPELINE DEL EQUIPO =====
#         try:
#             # Import din√°mico para evitar startup issues
#             from services.pii_detector import run_pipeline
#         except ImportError:
#             raise HTTPException(
#                 status_code=500, 
#                 detail="Pipeline de anonimizaci√≥n no disponible"
#             )

#         # Usar exactamente el mismo pipeline que usa el endpoint /anonymize
#         anonymization_result = run_pipeline(
#             model=request.model,
#             text=request.message,
#             use_regex=request.use_regex,
#             pseudonymize=request.pseudonymize,
#             save_mapping=False,  # Lo guardaremos nosotros en Redis si es necesario
#             use_realistic_fake=request.use_realistic_fake
#         )
        
#         anonymized_text = anonymization_result.get('anonymized', request.message)
#         mapping = anonymization_result.get('mapping', {})
#         pii_detected = bool(mapping)
        
#         logger.info(f"PII detectado: {pii_detected}, entidades: {len(mapping)}")
        
#         # ===== PASO 2: GUARDAR MAPA EN REDIS SI ES NECESARIO =====
#         if request.save_mapping and request.session_id and mapping:
#             try:
#                 from services.session_manager import store_anonymization_map
#                 store_anonymization_map(request.session_id, mapping)
#                 logger.info(f"Mapa guardado en Redis para sesi√≥n {request.session_id}")
#             except Exception as e:
#                 logger.warning(f"No se pudo guardar mapa en Redis: {e}")
        
#         # ===== PASO 3: ENV√çO AL LLM USANDO CLIENTE EXISTENTE =====
#         try:
#             from services.llm_integration import LLMClient
            
#             # Verificar que tenemos API key
#             import os
#             api_key = os.getenv("GROK_API_KEY") or os.getenv("SHIELD_AI_GROK_API_KEY")
#             if not api_key:
#                 raise HTTPException(
#                     status_code=500,
#                     detail="GROK_API_KEY no configurada en variables de entorno"
#                 )
                
#         except ImportError as e:
#             logger.error(f"Error importando LLMClient: {e}")
#             raise HTTPException(
#                 status_code=500,
#                 detail=f"Cliente LLM no disponible: {str(e)}"
#             )
        
#         llm_client = LLMClient()
        
#         # Preparar prompt para LLM
#         if request.llm_prompt_template:
#             llm_prompt = request.llm_prompt_template.format(text=anonymized_text)
#         else:
#             # Template por defecto para chat conversacional
#             llm_prompt = f"Act√∫a como un asistente √∫til y responde de manera clara y completa a la siguiente consulta: {anonymized_text}"
        
#         logger.info("Enviando consulta a Grok LLM...")
#         llm_response = llm_client.call_grok(llm_prompt)
        
#         # ===== PASO 4: DESANONIMIZACI√ìN USANDO SISTEMA PROPIO =====
#         final_response = llm_response
        
#         if pii_detected and mapping:
#             try:
#                 # Usar funci√≥n de desanonimizaci√≥n existente
#                 from services.deanonymization_service import create_reverse_map, deanonymize_text
                
#                 # Crear mapa inverso (datos_falsos -> datos_originales)
#                 reverse_map = create_reverse_map(mapping)
                
#                 # Desanonimizar respuesta del LLM
#                 final_response = deanonymize_text(llm_response, reverse_map)
                
#                 logger.info("Respuesta desanonimizada exitosamente")
                
#             except Exception as e:
#                 logger.error(f"Error en desanonimizaci√≥n: {e}")
#                 # Si falla desanonimizaci√≥n, devolver respuesta anonimizada
#                 final_response = llm_response
        
#         processing_time = time.time() - start_time
        
#         return ChatResponse(
#             response=final_response,
#             session_id=request.session_id,
#             pii_detected=pii_detected,
#             processing_time=round(processing_time, 3),
#             anonymized_used=pii_detected,
#             llm_model="mixtral-8x7b-32768"  # Modelo usado por LLMClient
#         )
        
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"Error en chat endpoint: {str(e)}")
#         raise HTTPException(
#             status_code=500, 
#             detail=f"Error procesando consulta: {str(e)}"
#         )

# === STREAMING CHAT ENDPOINT CON DUAL STREAM ===

@router.post("/streaming")
async def chat_stream_propuesta(request: ChatRequest):
    """
    Endpoint de chat con streaming dual que muestra:
    1. Stream an√≥nimo: Lo que ve el LLM (datos falsos)
    2. Stream deanonymizado: Lo que ve el usuario (datos reales)
    
    Combina chat + anonimizaci√≥n + LLM + dual streaming en un solo endpoint.
    """
    try:
        # ===== PASO 1: VERIFICAR SI EXISTE MAPA EN SESI√ìN =====
        session_id = request.session_id or f"stream_session_{int(time.time())}"
        existing_mapping = None
        
        logger.info(f"üîç INICIO - Sesi√≥n: {session_id}, Save mapping: {request.save_mapping}")
        
        if request.save_mapping and session_id:
            try:
                from services.session_manager import get_anonymization_map
                existing_mapping = get_anonymization_map(session_id)
                logger.info(f"‚úÖ MAPA EXISTENTE ENCONTRADO para sesi√≥n {session_id}: {len(existing_mapping)} entidades")
                logger.info(f"üìã Contenido del mapa: {existing_mapping}")
            except Exception as e:
                logger.info(f"‚ÑπÔ∏è  NO se encontr√≥ mapa existente para sesi√≥n {session_id}: {e}")
        
        # ===== PASO 2: ANONIMIZACI√ìN =====
        if existing_mapping:
            # USAR MAPA EXISTENTE para anonimizar
            logger.info(f"üîÑ USANDO MAPA EXISTENTE para anonimizar: '{request.message}'")
            anonymized_text = anonymize_with_existing_map(request.message, existing_mapping)
            mapping = existing_mapping
            pii_detected = True
            logger.info(f"‚úÖ Resultado anonimizaci√≥n con mapa existente: '{anonymized_text}'")
        else:
            # CREAR NUEVO MAPA usando pipeline del equipo
            logger.info(f"üÜï CREANDO NUEVO MAPA para: '{request.message}'")
            try:
                from services.pii_detector import run_pipeline
            except ImportError:
                raise HTTPException(
                    status_code=500, 
                    detail="Pipeline de anonimizaci√≥n no disponible"
                )

            anonymization_result = run_pipeline(
                model=request.model,
                text=request.message,
                use_regex=request.use_regex,
                pseudonymize=request.pseudonymize,
                save_mapping=False,
                use_realistic_fake=request.use_realistic_fake
            )
            
            anonymized_text = anonymization_result.get('anonymized', request.message)
            mapping = anonymization_result.get('mapping', {})
            pii_detected = bool(mapping)
            
            logger.info(f"‚úÖ Nuevo mapa creado: PII detectado: {pii_detected}, entidades: {len(mapping)}")
            logger.info(f"üìã Nuevo mapa contenido: {mapping}")
            logger.info(f"‚úÖ Resultado anonimizaci√≥n nuevo: '{request.message}' -> '{anonymized_text}'")
        
        # ===== PASO 3: GUARDAR MAPA EN REDIS =====
        if mapping and request.save_mapping:
            try:
                from services.session_manager import store_anonymization_map
                store_anonymization_map(session_id, mapping)
                logger.info(f"üíæ MAPA GUARDADO en Redis para sesi√≥n {session_id} con {len(mapping)} entidades")
            except Exception as e:
                logger.error(f"‚ùå ERROR guardando mapa en Redis: {e}")
        
        # ===== PASO 4: OBTENER RESPUESTA DEL LLM =====
        try:
            from services.llm_integration import LLMClientPropuesta
        except ImportError as e:
            logger.error(f"Error importando LLMClientPropuesta: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Cliente LLM Propuesta no disponible: {str(e)}"
            )
        
        llm_client = LLMClientPropuesta()
        
        if request.llm_prompt_template:
            llm_prompt = request.llm_prompt_template.format(text=anonymized_text)
        else:
            # Prompt ultra-directo para forzar uso de nombres exactos
            if mapping and pii_detected:
                # Extraer nombres del texto anonimizado
                import re
                names_in_text = []
                for fake_name in mapping.keys():
                    if fake_name in anonymized_text:
                        names_in_text.append(fake_name)
                
                if names_in_text:
                    names_str = ', '.join(f'"{name}"' for name in names_in_text)
                    llm_prompt = f"""SYSTEM: Eres un asistente. INSTRUCCI√ìN OBLIGATORIA: En tu respuesta usa √∫nicamente estos nombres exactos: {names_str}. No uses ning√∫n otro nombre.

USER: {anonymized_text}

Responde usando SOLO los nombres que aparecen en la consulta."""
                else:
                    llm_prompt = f"SYSTEM: Eres un asistente √∫til.\n\nUSER: {anonymized_text}"
            else:
                llm_prompt = f"Act√∫a como un asistente √∫til y responde de manera clara y completa a la siguiente consulta: {anonymized_text}"
        
        logger.info(f"üîç TEXTO ANONIMIZADO: '{anonymized_text}' (sesi√≥n: {session_id})")
        logger.info(f"üîç PROMPT COMPLETO ENVIADO AL LLM: '{llm_prompt}'")
        
        # ===== PASO 4.5: GUARDAR TEXTO ANONIMIZADO (REQUEST AL LLM) EN REDIS =====
        try:
            from services.session_manager import store_anonymized_request
            store_anonymized_request(session_id, anonymized_text)
            logger.info(f"üíæ TEXTO ANONIMIZADO guardado para sesi√≥n {session_id}")
        except Exception as e:
            logger.warning(f"No se pudo guardar texto anonimizado: {e}")
        
        # ===== PASO 5: INICIAR STREAMING REAL-TIME CON DEANONYMIZACI√ìN EN VIVO =====
        logger.info("üöÄ Iniciando streaming REAL-TIME del LLM con deanonymizaci√≥n en vivo...")
        from services.deanonymization_service import generate_real_time_dual_stream
        
        return StreamingResponse(
            generate_real_time_dual_stream(session_id, llm_prompt, mapping, llm_client),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "Content-Type"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error en chat streaming: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error en streaming: {str(e)}"
        )


# === DEBUG/TEST ENDPOINTS ===

@router.post("/test-anonymization-consistency")
async def test_anonymization_consistency(request: ChatRequest):
    """
    Endpoint de debug para probar consistencia de anonimizaci√≥n.
    √ötil para verificar que el bug est√© arreglado.
    """
    try:
        from services.session_manager import get_anonymization_map
        
        session_id = request.session_id or "test_consistency"
        
        # Verificar si existe mapa
        try:
            existing_map = get_anonymization_map(session_id)
            has_existing_map = True
        except:
            existing_map = None
            has_existing_map = False
        
        # Simular anonimizaci√≥n
        if has_existing_map:
            anonymized = anonymize_with_existing_map(request.message, existing_map)
            action = "used_existing_map"
        else:
            # Simular creaci√≥n de nuevo mapa (sin ejecutar pipeline completo)
            anonymized = request.message.replace("Juan P√©rez", "Mar√≠a Gonz√°lez").replace("Madrid", "Barcelona")
            action = "created_new_map"
            
            # Guardar mapa simple para futuras pruebas
            simple_map = {"Juan P√©rez": "Mar√≠a Gonz√°lez", "Madrid": "Barcelona"}
            try:
                from services.session_manager import store_anonymization_map
                store_anonymization_map(session_id, simple_map)
            except Exception as e:
                logger.warning(f"No se pudo guardar mapa simple: {e}")
        
        return {
            "session_id": session_id,
            "original_message": request.message,
            "anonymized_message": anonymized,
            "action": action,
            "has_existing_map": has_existing_map,
            "existing_map_size": len(existing_map) if existing_map else 0,
            "timestamp": time.time()
        }
        
    except Exception as e:
        logger.error(f"Error en test de consistencia: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error en test: {str(e)}"
        )

# === STREAMING CHAT ENDPOINT ===

# @router.post("/stream")
# async def chat_stream_with_llm(request: StreamingChatRequest):
#     """
#     Endpoint de streaming para chat en tiempo real.
#     Usa el mapa de anonimizaci√≥n guardado en la sesi√≥n.
#     """
#     try:
#         # Verificar que existe mapa de anonimizaci√≥n en la sesi√≥n
#         from services.session_manager import get_anonymization_map
        
#         anonymization_map = get_anonymization_map(request.session_id)
#         if not anonymization_map:
#             raise HTTPException(
#                 status_code=404,
#                 detail=f"No se encontr√≥ mapa de anonimizaci√≥n para sesi√≥n: {request.session_id}"
#             )
        
#         # Usar streaming de desanonimizaci√≥n existente con LLM
#         from services.deanonymization_service import generate_deanonymized_stream_get
        
#         return StreamingResponse(
#             generate_deanonymized_stream_get(request.session_id, anonymization_map),
#             media_type="text/event-stream",
#             headers={
#                 "Cache-Control": "no-cache",
#                 "Connection": "keep-alive",
#                 "Content-Type": "text/event-stream"
#             }
#         )
        
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"Error en streaming chat: {str(e)}")
#         raise HTTPException(
#             status_code=500,
#             detail=f"Error en streaming: {str(e)}"
#         )

# # === CHAT ENDPOINT CON LLM CLIENT PROPUESTA ===

# @router.post("/propuesta", response_model=ChatResponse)
# async def chat_with_llm_propuesta(request: ChatRequest):
#     """
#     Endpoint de chat usando LLMClientPropuesta mejorado:
#     1. Usa pipeline de anonimizaci√≥n del equipo (sin modificar)
#     2. Env√≠a prompt anonimizado al LLMClientPropuesta mejorado
#     3. Usa sistema de desanonimizaci√≥n propio para respuesta final
#     """
#     try:
#         start_time = time.time()
        
#         # ===== PASO 1: ANONIMIZACI√ìN USANDO PIPELINE DEL EQUIPO =====
#         try:
#             from services.pii_detector import run_pipeline
#         except ImportError:
#             raise HTTPException(
#                 status_code=500, 
#                 detail="Pipeline de anonimizaci√≥n no disponible"
#             )

#         anonymization_result = run_pipeline(
#             model=request.model,
#             text=request.message,
#             use_regex=request.use_regex,
#             pseudonymize=request.pseudonymize,
#             save_mapping=False,
#             use_realistic_fake=request.use_realistic_fake
#         )
        
#         anonymized_text = anonymization_result.get('anonymized', request.message)
#         mapping = anonymization_result.get('mapping', {})
#         pii_detected = bool(mapping)
        
#         logger.info(f"PII detectado: {pii_detected}, entidades: {len(mapping)}")
        
#         # ===== PASO 2: GUARDAR MAPA EN REDIS SI ES NECESARIO =====
#         if request.save_mapping and request.session_id and mapping:
#             try:
#                 from services.session_manager import store_anonymization_map
#                 store_anonymization_map(request.session_id, mapping)
#                 logger.info(f"Mapa guardado en Redis para sesi√≥n {request.session_id}")
#             except Exception as e:
#                 logger.warning(f"No se pudo guardar mapa en Redis: {e}")
        
#         # ===== PASO 3: ENV√çO AL LLM USANDO CLIENTE MEJORADO =====
#         try:
#             from services.llm_integration import LLMClientPropuesta
#         except ImportError as e:
#             logger.error(f"Error importando LLMClientPropuesta: {e}")
#             raise HTTPException(
#                 status_code=500,
#                 detail=f"Cliente LLM Propuesta no disponible: {str(e)}"
#             )
        
#         llm_client = LLMClientPropuesta()
        
#         # Preparar prompt para LLM
#         if request.llm_prompt_template:
#             llm_prompt = request.llm_prompt_template.format(text=anonymized_text)
#         else:
#             llm_prompt = f"Act√∫a como un asistente √∫til y responde de manera clara y completa a la siguiente consulta: {anonymized_text}"
        
#         logger.info("Enviando consulta a Grok LLM (Propuesta)...")
#         llm_response = llm_client.call_grok(llm_prompt)
        
#         # ===== PASO 4: DESANONIMIZACI√ìN USANDO SISTEMA PROPIO =====
#         final_response = llm_response
        
#         if pii_detected and mapping and not "[ERROR:" in llm_response:
#             try:
#                 from services.deanonymization_service import create_reverse_map, deanonymize_text
                
#                 # Debug: log del mapping para diagn√≥stico
#                 logger.info(f"Mapping original: {mapping}")
                
#                 reverse_map = create_reverse_map(mapping)
#                 logger.info(f"Reverse map: {reverse_map}")
                
#                 final_response = deanonymize_text(llm_response, reverse_map)
                
#                 logger.info("Respuesta desanonimizada exitosamente")
#                 logger.info(f"Respuesta antes: {llm_response[:100]}...")
#                 logger.info(f"Respuesta despu√©s: {final_response[:100]}...")
                
#             except Exception as e:
#                 logger.error(f"Error en desanonimizaci√≥n: {e}")
#                 final_response = llm_response
        
#         processing_time = time.time() - start_time
        
#         return ChatResponse(
#             response=final_response,
#             session_id=request.session_id,
#             pii_detected=pii_detected,
#             processing_time=round(processing_time, 3),
#             anonymized_used=pii_detected,
#             llm_model="llama3-8b-8192"  # Modelo usado por LLMClientPropuesta
#         )
        
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.error(f"Error en chat propuesta endpoint: {str(e)}")
#         raise HTTPException(
#             status_code=500, 
#             detail=f"Error procesando consulta: {str(e)}"
#         )

# # === ENDPOINT DE CONFIGURACI√ìN ===

# @router.post("/setup-test/{session_id}")
# async def setup_test_chat_session(session_id: str):
#     """
#     Configurar sesi√≥n de prueba para chat.
#     Usa la misma funcionalidad que el sistema de desanonimizaci√≥n.
#     """
#     try:
#         from services.deanonymization_service import dummy_anonymization_map
#         from services.session_manager import store_anonymization_map
        
#         # Usar el mapa dummy existente
#         dummy_map = dummy_anonymization_map()
#         store_anonymization_map(session_id, dummy_map)
        
#         return {
#             "message": f"Sesi√≥n de chat {session_id} configurada para pruebas",
#             "session_id": session_id,
#             "anonymization_map": dummy_map,
#             "chat_ready": True
#         }
        
#     except Exception as e:
#         raise HTTPException(
#             status_code=500,
#             detail=f"Error configurando sesi√≥n de prueba: {str(e)}"
#         )

# # === ENDPOINT DE STATUS ===

# @router.get("/status")
# async def chat_service_status():
#     """
#     Verificar estado de los servicios necesarios para chat.
#     """
#     status = {
#         "chat_service": "available",
#         "pii_detection": False,
#         "llm_client": False,
#         "llm_client_propuesta": False,
#         "deanonymization": False,
#         "redis_session": False,
#         "timestamp": time.time()
#     }
    
#     # Verificar PII detection
#     try:
#         from services.pii_detector import run_pipeline
#         status["pii_detection"] = True
#     except:
#         pass
    
#     # Verificar LLM client original
#     try:
#         from services.llm_integration import LLMClient
#         status["llm_client"] = True
#     except:
#         pass
    
#     # Verificar LLM client propuesta
#     try:
#         from services.llm_integration import LLMClientPropuesta
#         status["llm_client_propuesta"] = True
#     except:
#         pass
    
#     # Verificar desanonimizaci√≥n
#     try:
#         from services.deanonymization_service import deanonymize_text
#         status["deanonymization"] = True
#     except:
#         pass
    
#     # Verificar Redis
#     try:
#         from services.session_manager import get_anonymization_map
#         status["redis_session"] = True
#     except:
#         pass
    
#     status["all_services_ready"] = all([
#         status["pii_detection"],
#         status["llm_client"], 
#         status["deanonymization"],
#         status["redis_session"]
#     ])
    
#     status["all_services_ready_propuesta"] = all([
#         status["pii_detection"],
#         status["llm_client_propuesta"], 
#         status["deanonymization"],
#         status["redis_session"]
#     ])
    
#     return status

# # === ENDPOINT PARA PROBAR LLM CLIENT PROPUESTA ===

# @router.post("/test-llm-propuesta")
# async def test_llm_client_propuesta(request: dict):
#     """
#     Endpoint para probar la nueva clase LLMClientPropuesta.
#     """
#     try:
#         from services.llm_integration import LLMClientPropuesta
        
#         client = LLMClientPropuesta()
        
#         # Test de conexi√≥n
#         connection_test = client.test_connection()
        
#         # Test con prompt personalizado si se proporciona
#         prompt = request.get("prompt", "Hola, ¬øpuedes responder en espa√±ol?")
        
#         if connection_test["status"] == "success":
#             start_time = time.time()
#             response = client.call_grok(prompt)
#             processing_time = time.time() - start_time
            
#             return {
#                 "status": "success",
#                 "connection_test": connection_test,
#                 "prompt": prompt,
#                 "response": response,
#                 "processing_time": round(processing_time, 3),
#                 "model_used": client.model
#             }
#         else:
#             return {
#                 "status": "connection_failed",
#                 "connection_test": connection_test,
#                 "prompt": prompt
#             }
            
#     except Exception as e:
#         return {
#             "status": "error",
#             "error": str(e),
#             "prompt": request.get("prompt", "")
#         }