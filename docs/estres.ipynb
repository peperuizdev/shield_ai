{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d579c47",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2) Ubicación y explicación de endpoints\n",
    "\n",
    "- `/api/anonymize`  \n",
    "  ↳ **Archivo:** anonymization.py  \n",
    "  ↳ **Qué hace:** Anonimiza texto con PII.  \n",
    "  ↳ **Qué mide:** Tiempo de respuesta y errores al anonimizar.\n",
    "\n",
    "- `/api/deanonymize`  \n",
    "  ↳ **Archivo:** deanonymization.py  \n",
    "  ↳ **Qué hace:** Desanonimiza texto usando mapping.  \n",
    "  ↳ **Qué mide:** Tiempo de respuesta y errores al revertir.\n",
    "\n",
    "- `/api/chat/streaming`  \n",
    "  ↳ **Archivo:** chat.py  \n",
    "  ↳ **Qué hace:** Simula interacción de chat con streaming.  \n",
    "  ↳ **Qué mide:** Capacidad de respuesta en tiempo real bajo carga.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Recorrido de la estructura de carpetas\n",
    "\n",
    "- **routes**: Aquí están los endpoints principales de la API.\n",
    "- **services**: Lógica de negocio para anonimización y desanonimización.\n",
    "- **core**: Configuración y utilidades del backend.\n",
    "- **docs**: Aquí se guardará el reporte de estrés generado por Locust.\n",
    "- **`tests/`**: Lugar donde se ejecuta Locust y se ubica el `locustfile.py`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Métricas analizadas\n",
    "\n",
    "| **Métrica**                | **Descripción**                                                                 | **Cómo se mide**                                  | **Significado**                                                                 |\n",
    "|----------------------------|--------------------------------------------------------------------------------|---------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| Total Requests             | Número total de peticiones realizadas durante la prueba                         | Contador global                                   | Carga total soportada                                                            |\n",
    "| Total Failures             | Número de peticiones fallidas (errores HTTP o excepciones)                      | Contador global                                   | Estabilidad y robustez del sistema                                               |\n",
    "| Max Response Time (ms)     | Tiempo máximo de respuesta observado                                            | Medido por Locust                                 | Indica si hay cuellos de botella o lentitud bajo carga                           |\n",
    "| Min Response Time (ms)     | Tiempo mínimo de respuesta observado                                            | Medido por Locust                                 | Mejor caso de respuesta                                                          |\n",
    "| Test Duration (s)          | Duración total de la prueba                                                    | Diferencia de tiempo entre inicio y fin            | Permite calcular tasas y promedios                                               |\n",
    "| Requests per Second        | Promedio de peticiones por segundo                                             | Total Requests / Test Duration                    | Capacidad de throughput del backend                                              |\n",
    "| Failure Rate (%)           | Porcentaje de peticiones fallidas                                              | (Total Failures / Total Requests) * 100           | Si es alto, indica problemas bajo carga                                          |\n",
    "| Estimated Max Users        | Estimación de usuarios concurrentes soportados                                 | Heurística basada en tiempos y errores            | Si el sistema mantiene <1s y <2% errores, se considera soportado                 |\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Reporte automático\n",
    "\n",
    "- Al finalizar la prueba, Locust generará un archivo llamado  \n",
    "  `stress_report.txt`  \n",
    "  en  \n",
    "  docs\n",
    "- El reporte incluirá todas las métricas y una conclusión sobre la cantidad de usuarios concurrentes soportados.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Ejecución\n",
    "\n",
    "1. Instala Locust si no lo tienes:\n",
    "   ```\n",
    "   pip install locust\n",
    "   ```\n",
    "2. Desde la terminal, navega a:\n",
    "   ```\n",
    "   cd c:\\Users\\admin\\Desktop\\SHIELD-AI\\proyecto_final\\shield_ai\\backend\\tests\n",
    "   ```\n",
    "3. Ejecuta Locust:\n",
    "   ```\n",
    "   locust -f locustfile.py --host=http://localhost:8000\n",
    "   ```\n",
    "   (Ajusta el host si tu backend corre en otro puerto o dirección)\n",
    "\n",
    "4. Abre tu navegador en [http://localhost:8089](http://localhost:8089), elige el número de usuarios y comienza la prueba.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) ¿Cómo interpretar el reporte?\n",
    "\n",
    "- Si el reporte indica que el **Max Response Time** es bajo (<1000ms) y el **Failure Rate** es bajo (<2%), tu sistema soporta bien la cantidad de usuarios concurrentes probados.\n",
    "- Si ves muchos errores o tiempos altos, ese es tu límite actual.\n",
    "\n",
    "---\n",
    "\n",
    "¿Quieres personalizar el test para otros endpoints o escenarios? ¿Necesitas ayuda para interpretar los resultados?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9b059",
   "metadata": {},
   "source": [
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4f9b5",
   "metadata": {},
   "source": [
    "¡Claro! Aquí tienes una explicación **paso a paso** de qué poner en cada campo de la pantalla de Locust que ves en la imagen:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Number of users (peak concurrency)**\n",
    "- **¿Qué es?**  \n",
    "  Es el número máximo de usuarios simulados que estarán usando la aplicación al mismo tiempo.\n",
    "- **¿Qué poner?**  \n",
    "  - Para una prueba inicial, puedes poner `1` o `5`.\n",
    "  - Para pruebas de estrés, aumenta progresivamente: `10`, `20`, `50`, `100`, etc.\n",
    "- **¿Cómo influye?**  \n",
    "  Más usuarios = más carga sobre tu backend. Así descubres cuántos usuarios soporta tu sistema antes de degradarse.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Ramp up (users started/second)**\n",
    "- **¿Qué es?**  \n",
    "  Es la velocidad con la que Locust agregará nuevos usuarios simulados por segundo.\n",
    "- **¿Qué poner?**  \n",
    "  - Para pruebas suaves, pon `1` (un usuario nuevo por segundo).\n",
    "  - Para pruebas más agresivas, puedes poner `5`, `10`, etc.\n",
    "- **¿Cómo influye?**  \n",
    "  Si pones un número alto, la carga sube rápido y puedes ver cómo responde tu sistema ante picos de tráfico.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Host**\n",
    "- **¿Qué es?**  \n",
    "  Es la URL base de tu backend, donde Locust enviará las peticiones.\n",
    "- **¿Qué poner?**  \n",
    "  - Si tu backend corre localmente, deja `http://localhost:8000`.\n",
    "  - Si está en otro puerto o en la nube, pon la URL correspondiente (ejemplo: `http://192.168.1.10:8000` o la IP/URL de tu servidor).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Advanced options**\n",
    "- **¿Qué es?**  \n",
    "  Opciones avanzadas como tiempo máximo de espera, número de peticiones, etc.\n",
    "- **¿Qué poner?**  \n",
    "  - Puedes dejarlo vacío para la mayoría de pruebas.\n",
    "  - Si quieres limitar la duración o el número de peticiones, puedes configurarlo aquí.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **START**\n",
    "- Haz clic en **START** para comenzar la prueba de carga con los parámetros que elegiste.\n",
    "\n",
    "---\n",
    "\n",
    "## **¿Qué sucede después?**\n",
    "- Locust empezará a simular usuarios y verás en tiempo real:\n",
    "  - **RPS**: Requests per second (peticiones por segundo).\n",
    "  - **Failures**: Porcentaje de errores.\n",
    "  - **Response Times**: Tiempos de respuesta promedio, mínimo y máximo.\n",
    "- Cuando termines, Locust y tu script generarán el reporte en el archivo `stress_report.txt` en la carpeta docs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Consejo**\n",
    "- Empieza con pocos usuarios y ve subiendo poco a poco.\n",
    "- Observa cuándo empiezan a aumentar los errores o los tiempos de respuesta: ese es el límite de tu sistema.\n",
    "\n",
    "---\n",
    "\n",
    "¿Quieres una recomendación de valores para tu primer test?  \n",
    "Por ejemplo:  \n",
    "- Number of users: `10`  \n",
    "- Ramp up: `2`  \n",
    "- Host: `http://localhost:8000`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89543d6b",
   "metadata": {},
   "source": [
    "# **********************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8f1540",
   "metadata": {},
   "source": [
    "Aquí tienes un **plan paso a paso (pipeline)** para probar tu aplicación con Locust, empezando desde el mínimo hasta el máximo, y cómo interpretar cada etapa:\n",
    "\n",
    "---\n",
    "\n",
    "## **Pipeline de Pruebas de Estrés con Locust**\n",
    "\n",
    "### **1. Prueba de base (mínima carga)**\n",
    "- **Number of users:** 1\n",
    "- **Ramp up:** 1\n",
    "- **¿Para qué?**  \n",
    "  Verifica que todo funciona correctamente con un solo usuario.  \n",
    "  **¿Qué observar?**  \n",
    "  - El sistema responde bien, sin errores.\n",
    "  - Tiempos de respuesta bajos (<300ms).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Prueba baja concurrencia**\n",
    "- **Number of users:** 5\n",
    "- **Ramp up:** 1\n",
    "- **¿Para qué?**  \n",
    "  Simula un pequeño grupo de usuarios concurrentes.\n",
    "  **¿Qué observar?**  \n",
    "  - ¿Aumentan los tiempos de respuesta?\n",
    "  - ¿Aparecen errores?\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Prueba de concurrencia media**\n",
    "- **Number of users:** 10\n",
    "- **Ramp up:** 2\n",
    "- **¿Para qué?**  \n",
    "  Simula una situación realista de uso.\n",
    "  **¿Qué observar?**  \n",
    "  - ¿El sistema sigue estable?\n",
    "  - ¿El Failure Rate sigue bajo (<2%)?\n",
    "  - ¿El Max Response Time sigue <1s?\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Prueba de pico moderado**\n",
    "- **Number of users:** 25\n",
    "- **Ramp up:** 5\n",
    "- **¿Para qué?**  \n",
    "  Simula un pico de tráfico moderado.\n",
    "  **¿Qué observar?**  \n",
    "  - ¿Empiezan a subir los errores?\n",
    "  - ¿El sistema se mantiene usable?\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Prueba de estrés alto**\n",
    "- **Number of users:** 50\n",
    "- **Ramp up:** 10\n",
    "- **¿Para qué?**  \n",
    "  Lleva el sistema cerca de su límite.\n",
    "  **¿Qué observar?**  \n",
    "  - ¿Cuándo empiezan a aumentar mucho los tiempos de respuesta?\n",
    "  - ¿Cuándo el Failure Rate supera el 2%?\n",
    "  - ¿Aparecen errores 500, 502, 429?\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Prueba de saturación**\n",
    "- **Number of users:** 100\n",
    "- **Ramp up:** 20\n",
    "- **¿Para qué?**  \n",
    "  Descubre el punto de saturación total.\n",
    "  **¿Qué observar?**  \n",
    "  - El sistema probablemente colapsa o responde muy lento.\n",
    "  - Este es tu “límite máximo”.\n",
    "\n",
    "---\n",
    "\n",
    "## **¿Cómo avanzar?**\n",
    "\n",
    "1. **Empieza con la prueba mínima.**\n",
    "2. **Aumenta usuarios y ramp up según el plan.**\n",
    "3. **Después de cada prueba, revisa el reporte (`stress_report.txt`).**\n",
    "4. **Anota en qué punto los tiempos de respuesta o errores se disparan.**\n",
    "5. **Ese es tu límite realista de usuarios concurrentes.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Consejo**\n",
    "- Si quieres ser más preciso, repite cada prueba varias veces y promedia los resultados.\n",
    "- Si tu sistema empieza a fallar antes de lo esperado, revisa logs y recursos del servidor.\n",
    "\n",
    "---\n",
    "\n",
    "¿Quieres una tabla con los valores para copiar y pegar? ¿O una plantilla para registrar los resultados?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cd3bdb",
   "metadata": {},
   "source": [
    "# ******************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd34bf30",
   "metadata": {},
   "source": [
    "Eso que ves en la sección **Advanced options** de Locust es para configurar detalles adicionales de la prueba de carga:\n",
    "\n",
    "---\n",
    "\n",
    "### **Run time**\n",
    "- **¿Qué es?**  \n",
    "  Especifica cuánto tiempo quieres que dure la prueba de carga.\n",
    "- **¿Cómo se usa?**  \n",
    "  Escribe el tiempo en formato como:  \n",
    "  - `20` (20 segundos)\n",
    "  - `20s` (20 segundos)\n",
    "  - `3m` (3 minutos)\n",
    "  - `1h20m` (1 hora y 20 minutos)\n",
    "  - `3h30m10s` (3 horas, 30 minutos y 10 segundos)\n",
    "- **¿Para qué sirve?**  \n",
    "  Si lo dejas vacío, la prueba seguirá hasta que la detengas manualmente.  \n",
    "  Si pones un valor, Locust parará automáticamente cuando pase ese tiempo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Profile**\n",
    "- **¿Qué es?**  \n",
    "  Permite seleccionar un perfil de prueba predefinido (si tienes varios escenarios guardados).\n",
    "- **¿Para qué sirve?**  \n",
    "  Si no tienes perfiles configurados, puedes dejarlo vacío.\n",
    "\n",
    "---\n",
    "\n",
    "**En resumen:**  \n",
    "- Usa **Run time** para limitar la duración de la prueba automáticamente.\n",
    "- Usa **Profile** solo si tienes varios escenarios de prueba guardados (opcional).\n",
    "\n",
    "¿Quieres un ejemplo de cómo llenarlo?  \n",
    "Por ejemplo:  \n",
    "- Run time: `2m` (para una prueba de 2 minutos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652b945",
   "metadata": {},
   "source": [
    "# ******************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4600f",
   "metadata": {},
   "source": [
    "Claro, aquí tienes el resumen sencillo para explicarlo:\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Cuál era el error?**\n",
    "El error era que el backend devolvía un **400 Bad Request** diciendo:  \n",
    "**\"Debe proporcionar 'message' o 'file'\"**  \n",
    "Esto pasaba porque el endpoint esperaba los datos en formato formulario, no en JSON.\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Por qué fuimos a ver `chat.py`?**\n",
    "Fuimos a revisar `chat.py` para ver **cómo el endpoint `/chat/streaming` recibía los datos**.  \n",
    "Ahí vimos que usaba `Form` y `File`, lo que significa que espera **campos tipo formulario** (`multipart/form-data`), no JSON.\n",
    "\n",
    "---\n",
    "\n",
    "### **¿Qué se modificó en `locustfile.py`?**\n",
    "Cambiamos la línea:\n",
    "```python\n",
    "self.client.post(\"/chat/streaming\", json={...})\n",
    "```\n",
    "por:\n",
    "```python\n",
    "self.client.post(\"/chat/streaming\", data={...})\n",
    "```\n",
    "Así Locust envía los datos como formulario, que es lo que el backend espera.\n",
    "\n",
    "---\n",
    "\n",
    "**En resumen:**  \n",
    "El error era por el formato de los datos enviados.  \n",
    "Revisamos el backend para ver el formato correcto.  \n",
    "Modificamos Locust para enviar los datos como formulario y así todo funcionó bien."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
